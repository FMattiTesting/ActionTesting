
\section{Randomized trace estimators for parameter-dependent matrices}
\label{sec:analysis}

\color{blue}
In this section, we consider a general parameter-dependent matrix
\begin{equation*}
    \mtx{B}(t) = \begin{bmatrix}
        b_{11}(t) & b_{12}(t) & \dots & b_{1n}(t) \\
        b_{21}(t) & b_{22}(t) & \dots & b_{2n}(t) \\
        \vdots & \vdots & \ddots & \vdots \\
        b_{n1}(t) & b_{n2}(t) & \dots & b_{nn}(t) \\
    \end{bmatrix} \in \mathbb{R}^{n \times n},
\end{equation*}
where each entry $b_{ij}(t)$ is a continuous function on a bounded interval $[a,b]$. \emph{If} these entries are given explicitly, we can simply compute the trace by its definition 
$\Trace(\mtx{B}(t)) = b_{11}(t)+ \cdots + b_{nn}(t)$. This becomes more difficult when $\mtx{B}(t)$ is given implicitly (e.g., as a matrix function) and can only be accessed through matrix-vector products. The common theme of all methods analyzed in this section is to extract trace estimates from multiplying $\mtx{B}(t)$ with fixed random vectors

as we will see in Section ???.
\color{black}

\subsection{Methods}
\label{subsec:methods}

\paragraph{Girard-Hutchinson estimator.} The trace of a parameter-dependent matrix can be approximated with the Girard-Hutchinson estimator \cite{girard-1989-fast-montecarlo,hutchinson-1990-stochastic-estimator}: We take $n_{\mtx{\Psi}}$ stochastically independent standard Gaussian random vectors $\vct{\psi}_1,\dots, \vct{\psi}_{n_{\mtx{\Psi}}} \in \mathbb{R}^{n}$, i.e. vectors whose entries are independent standard Gaussian random variables, to form
\begin{equation}
    \Hutch{\mtx{\Psi}}(\mtx{B}(t))
    = \frac{1}{n_{\mtx{\Psi}}} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j
    = \frac{1}{n_{\mtx{\Psi}}} \Trace( \mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi})
    \label{equ:hutchinson-trace-estimator}
\end{equation}
where $\mtx{\Psi} = [\vct{\psi}_1 ~ \cdots ~ \vct{\psi}_{n_{\mtx{\Psi}}}] \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$ is a standard Gaussian random matrix. Other choices for the distribution of the random vectors are possible, for example by uniformly sampling from $\{-1, +1\}$ or from the $(n-1)$-sphere. Our theoretical developments only hold in the Gaussian case.

The Girard-Hutchinson estimator distinguishes itself for its simplicity. It has already been used to approximate traces of parameter-dependent matrices for spectral density estimation in the Haydock method \cite{haydock-1972-electronic-structure}, as well as the Delta-Gauss-Legendre and Delta-Gauss-Chebyshev methods of \cite{lin-2016-approximating-spectral, lin-2017-randomized-estimation}. However, as a Monte Carlo estimate, its accuracy is often quite disappointing: if we aim to increase the accuracy of the estimate by one digit, we would need to increase the number of queries by a factor of one hundred.

\paragraph{Nyström estimator.} Matrices with fast singular value decay can well be approximated with a low-rank approximation. Intuitively, it should be sufficient to approximate the trace of a quadratic matrix with the trace of a good low-rank approximation. Our application, the spectral density estimation (\refsec{sec:application}) of symmetric matrices, requires us to approximate the trace of a symmetric positive semi-definite matrix, originating from a non-negative smoothing kernel. In this scenario, the Nyström approximation is the most suitable low-rank approximation \cite{gittens-2013-revisiting-nystrom}. For a standard Gaussian random matrix $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$, it is defined as
\begin{equation}
    \Nystr{\mtx{\Omega}}{\mtx{B}}(t) = (\mtx{B}(t) \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}(t) \mtx{\Omega})^{\dagger} (\mtx{B}(t) \mtx{\Omega})^{\top}.
    \label{equ:nystrom-approximation}
\end{equation}
We can estimate the trace of $\mtx{B}(t)$ with $\Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t))$. Thanks to the invariance of the trace under cyclic permutation of its arguments and the symmetry of the matrix, we may rewrite this estimator as
\begin{equation}
    \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) = \Trace( (\mtx{\Omega}^{\top} \mtx{B}(t) \mtx{\Omega})^{\dagger} ( \mtx{\Omega}^{\top} \mtx{B}(t)^2 \mtx{\Omega})).
    \label{equ:nystrom-trace-estimator}
\end{equation}
This estimator forms the basis of the so-called spectrum sweeping method \cite{lin-2017-randomized-estimation}. 

Two different low-rank approximation algorithms for general parameter-dependent matrices have already been analyzed in \cite{kressner-2024-randomized-lowrank}. However, the downside of the low-rank approximation approach is that it will -- in general -- not work well enough on matrices whose singular values do not decay quickly.
%Write:
%\begin{itemize}
% \item fast singular value decay, intuitively sufficient to consider a low-rank %approximation
% \item motivated by our application we only consider symmetric positive %semi-definite case here (smoothing kernels are positive) for which Nystroem %approximation is the right choice
%\end{itemize}

%\textcolor{red}{Moved from below:
%For symmetric positive semi-definite matrices whose singular values decay %rapidly, the Nyström estimate \refequ{equ:nystrom-trace-estimator} can be %significantly better than the Girard-Hutchinson estimate. This is reflected in %the following theorem.}
%
%
%Alternatively, the trace of a symmetric matrix whose singular values decay %quickly can be approximated well by using a Gaussian sketching matrix $\mtx%{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ to form the Nyström %approximation \cite{gittens-2013-revisiting-nystrom}

\paragraph{Nyström++ estimator.} Finally, \cite{lin-2017-randomized-estimation} proposes an estimator which is more accurate than the Girard-Hutchinson estimator and, unlike the Nyström approximation, does not rely on a fast singular value decay of $\mtx{B}(t)$. The estimator first computes the trace of the Nyström approximation \refequ{equ:nystrom-trace-estimator} of the matrix. Because this result may not always be satisfactory, the estimator then corrects for eventual inaccuracies in the Nyström approximation by estimating the trace of the approximation residual with the Girard-Hutchinson estimator \refequ{equ:hutchinson-trace-estimator}. The full estimator is
\begin{equation}
    \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) = \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) + \Hutch{\mtx{\Psi}}(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)),
    \label{equ:nystrompp-trace-estimator}
\end{equation}
where the matrix indices again emphasize the use of a constant standard Gaussian random matrix $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$ for the Girard-Hutchinson estimator and the dimension reduction matrix $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ for the Nyström approximation.

This estimator is the parameter-dependent analogue of the Nyström++ estimator \cite{persson-2022-improved-variants}, which itself is based on the Hutch++ estimator \cite{meyer-2021-hutch-optimal}. For constant matrices $\mtx{B}$, these estimators were both shown to achieve a relative $\varepsilon$-error with just using $\mathcal{O}(\varepsilon^{-1})$ matrix-vector products, independent of the singular value decay of $\mtx{B}$.
%We can interpret this estimator as an interpolation between the trace of the Nyström approximation and the Girard-Hutchinson estimator.

%In the remainder of this section, we will derive upper bounds on the error of these estimators.
%
%\begin{table}[ht]
%\centering
%\renewcommand{\arraystretch}{1.2}
%\begin{tabular}{@{}lcc@{}}
%\toprule
%Estimator & Matrix & $m=1600$\\
%\midrule
%Girard-Hutchinson & symmetric & $\mathcal{O}(\varepsilon^{-2})%$ \\
%Nyström & symmetric positive semi-definite & $\mathcal{O}%(\varepsilon^{-2})$ \\
%Nyström++ & symmetric positive semi-definite & $\mathcal{O}%(\varepsilon^{-1})$  \\
%\bottomrule
%\end{tabular}
%\end{table}

\subsection{Error bounds}

\textcolor{green}{In the following pages, we will upper bound the $L^1$-error of each of the trace estimators from \refsec{subsec:methods} with a suitable quantity which parallels the corresponding result in the constant case. As in \cite{kressner-2024-randomized-lowrank}, this often requires us to bound the high order moments $\mathbb{E}^p[X] = (\mathbb{E}[|X|^p])^{\sfrac{1}{p}}$ of certain random variables $X$.}

\subsubsection{Girard-Hutchinson estimator}
\label{subsec:hutchinson}

\color{blue}

We start our analysis of the methods presented above by deriving an $L^1$-error bound for the Girard-Hutchinson estimator~\refequ{equ:hutchinson-trace-estimator} that is relative to the $L^1$-norm of the Frobenius norm $\lVert \mtx{B}(t) \rVert _F$.

%\begin{theorem}{Girard-Hutchinson estimator for parameter-dependent matrices}{hutchinson}
%    For a symmetric matrix $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ that depends continuously on $t \in [a, b]$, consider the  Girard-Hutchinson estimator~\refequ{equ:hutchinson-trace-estimator} with an $n\times n_{\mtx{\Psi}}$ Gaussian random matrix $\mtx{\Psi}$. Then for any $p \geq 1, p \in \mathbb{R}$ and $\gamma \geq 1$ the bound 
%    \begin{equation} \label{eq:ghbound}
%        \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t \leq c \gamma \max\left\{ \sqrt{\frac{p}{n_{\mtx{\Psi}}}}, \frac{p}{n_{\mtx{\Psi}}}  \right\} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t.
%    \end{equation}
%    holds with probability at least $1 - \gamma^{-p}$.
%    In particular, given $\varepsilon \in (0, ce)$ and $\delta \in (0, e^{-1})$, the bound $\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) | ~\mathrm{d}t \leq \varepsilon \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t$ holds 
%    with probability at least $1-\delta$ if $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2} \log(\delta^{-1}))$.
%\end{theorem}

\begin{theorem}{Girard-Hutchinson estimator for parameter-dependent matrices}{hutchinson}
    For a symmetric matrix $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ that depends continuously on $t \in [a, b]$, consider the  Girard-Hutchinson estimator~\refequ{equ:hutchinson-trace-estimator} with an $n\times n_{\mtx{\Psi}}$ Gaussian random matrix $\mtx{\Psi}$. Then for any $p \geq 1, p \in \mathbb{R}$ and $\gamma \geq 1$ the bound 
    \begin{equation} \label{eq:ghbound}
        \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t \leq \gamma c \left( \sqrt{\frac{p}{n_{\mtx{\Psi}}}} + \frac{p}{n_{\mtx{\Psi}}}  \right) \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t
    \end{equation}
    holds with probability at least $1 - \gamma^{-p}$ and for $c = 4 (1 + \sqrt{1 + \pi/4})$.
    In particular, given $\varepsilon \in (0, 1)$ and $\delta \in (0, e^{-1})$, the bound $\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) | ~\mathrm{d}t \leq \varepsilon \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t$ holds 
    with probability at least $1-\delta$ if $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2} \log(\delta^{-1}))$.
\end{theorem}

\color{black}
\begin{proof}
    The proof proceeds by bounding moments for fixed values of $t$ and then uses Minkowski's integral inequality. We start with a standard embedding trick (see, e.g.,~\cite[Theorem 1]{cortinovis-2022-randomized-trace}) for which we define
    \begin{equation}
        \widetilde{\mtx{B}}(t)
        = \frac{1}{n_{\mtx{\Psi}}} \begin{pmatrix}
            \mtx{B}(t) & & \\
            & \ddots & \\
            & & \mtx{B}(t)
        \end{pmatrix}
        \quad \text{and} \quad
        \widetilde{\vct{\psi}} = \begin{pmatrix}
            \vct{\psi}_1 \\
            \vdots \\
            \vct{\psi}_{n_{\mtx{\Psi}}}
        \end{pmatrix}.
    \end{equation}
    Note that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) =  \Trace(\widetilde{\mtx{B}}(t)) - \widetilde{\vct{\psi}}^{\top} \widetilde{\mtx{B}}(t) \widetilde{\vct{\psi}}$, $\lVert \widetilde{\mtx{B}}(t) \rVert _2 = \lVert \mtx{B}(t) \rVert _2 / n_{\mtx{\Psi}}$ and $\lVert \widetilde{\mtx{B}}(t) \rVert _F = \lVert \mtx{B}(t) \rVert _F / \sqrt{n_{\mtx{\Psi}}}$ hold. Therefore, by \reflem{lem:hutchinson-sub-exponential}, $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$ is a centered and $(2 \lVert \mtx{B}(t) \rVert _F^2 (1 - 1/\beta)^{-1} / n_{\mtx{\Psi}}, 2 \lVert \mtx{B}(t) \rVert _2 \beta / n_{\mtx{\Psi}})$-sub-exponential random variable for any $\beta > 1$. Thus, we can apply \reflem{lem:sub-exponential-moments} to bound
    \begin{equation}
        \mathbb{E}^{p}[\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))] \leq 2 \left(\sqrt{\frac{\pi}{1-1/\beta}} \sqrt{\frac{p}{n_{\mtx{\Psi}}}} \lVert \mtx{B}(t) \rVert _F + 4 \beta \frac{p}{n_{\mtx{\Psi}}} \lVert \mtx{B}(t) \rVert _2 \right).
    \end{equation}
    Bounding $\lVert \mtx{B}(t) \rVert _2 \leq \lVert \mtx{B}(t) \rVert _F$ and letting $\beta = (1 + \sqrt{1 + \pi/4}) / 2$ we get
    \begin{equation}
        \mathbb{E}^{p}[\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))] \leq \underbrace{4 (1 + \sqrt{1 + \pi/4})}_{=~c} \left( \sqrt{\frac{p}{n_{\mtx{\Psi}}}} + \frac{p}{n_{\mtx{\Psi}}} \right) \lVert \mtx{B}(t) \rVert _F.
        \label{equ:moment-bound-hutchinson}
    \end{equation}

    To address the parameter-dependent case, we first note that the continuity assumption implies that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$ is measurable. Therefore, we can apply Minkowski's integral inequality~\cite[Theorem 202]{hardy-1952-inequalities} to conclude from the moment bound~\refequ{equ:moment-bound-hutchinson} that
    \begin{align}
        \mathbb{E}^{p}\left[ \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t  \right]
        &\leq \int_{a}^{b} \mathbb{E}^{p}\left[ \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) \right]~\mathrm{d}t \notag \\
        &\leq c \left( \sqrt{\frac{p}{n_{\mtx{\Psi}}}} + \frac{p}{n_{\mtx{\Psi}}}  \right) \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t .
    \end{align}
    Consequently, by Markov's inequality, the bound 
    \begin{equation} 
        \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t \leq \gamma c \left( \sqrt{\frac{p}{n_{\mtx{\Psi}}}} + \frac{p}{n_{\mtx{\Psi}}}  \right) \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t
    \end{equation}
    holds for any $\gamma \geq 1$ with probability at least $1 - \gamma^{-p}$. 

    Setting $\gamma = e$ and choosing $p = \log(\delta^{-1}) \geq 1$, the second part of the theorem follows when taking $n_{\mtx{\Psi}} = \lceil 2 c^2 e^2 \log(\delta^{-1}) \varepsilon^{-2} \rceil$.
\end{proof}
%\begin{proof}
%    The proof proceeds by bounding moments for fixed values of $t$ and then uses Minkowski's integral inequality. We start with a standard embedding trick (see, e.g.,~\cite[Theorem 1]{cortinovis-2022-randomized-trace}) for which we define
%    \begin{equation}
%        \widetilde{\mtx{B}}(t)
%        = \frac{1}{n_{\mtx{\Psi}}} \begin{pmatrix}
%            \mtx{B}(t) & & \\
%            & \ddots & \\
%            & & \mtx{B}(t)
%        \end{pmatrix}
%        \quad \text{and} \quad
%        \widetilde{\vct{\psi}} = \begin{pmatrix}
%            \vct{\psi}_1 \\
%            \vdots \\
%            \vct{\psi}_{n_{\mtx{\Psi}}}
%        \end{pmatrix}.
%    \end{equation}
%    Note that $\Trace(\widetilde{\mtx{B}}(t)) - \widetilde{\vct{\psi}}^{\top} \widetilde{\mtx{B}}(t) \widetilde{\vct{\psi}} = \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$, $\lVert \widetilde{\mtx{B}}(t) \rVert _2 = \lVert \mtx{B}(t) \rVert _2 / n_{\mtx{\Psi}}$ and $\lVert \widetilde{\mtx{B}}(t) \rVert _F = \lVert \mtx{B}(t) \rVert _F / \sqrt{n_{\mtx{\Psi}}}$ hold. Therefore, by \reflem{lem:hanson-wright-moments} we have
%    \begin{align}
%        \mathbb{E}^{p}[\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))]
%        &= \mathbb{E}^{p}[\Trace(\widetilde{\mtx{B}}(t)) - \widetilde{\vct{\psi}}^{\top} \widetilde{\mtx{B}}(t) \widetilde{\vct{\psi}}] \notag \\
%        &\leq c \max\left\{ \sqrt{p} \lVert \widetilde{\mtx{B}}(t) \rVert _F, p \lVert \widetilde{\mtx{B}}(t) \rVert _2 \right\} \notag \\
%        &= c \max\left\{ \sqrt{\frac{p}{n_{\mtx{\Psi}}}} \lVert \mtx{B}(t) \rVert _F, \frac{p}{n_{\mtx{\Psi}}} \lVert \mtx{B}(t) \rVert _2 \right\} \notag \\
%        &\leq c \max\left\{ \sqrt{\frac{p}{n_{\mtx{\Psi}}}}, \frac{p}{n_{\mtx{\Psi}}} \right\}  \lVert \mtx{B}(t) \rVert _F.
%        \label{equ:moment-bound-hanson-wright}
%    \end{align}
%
%    To address the parameter-dependent case, we first note that the continuity assumption implies that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$ is measurable. Therefore, we can apply Minkowski's integral inequality~\cite[Theorem 202]{hardy-1952-inequalities} to conclude from~\refequ{equ:moment-bound-hanson-wright} that
%    \begin{align}
%        \mathbb{E}^{p}\left[ \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t  \right]
%        &\leq \int_{a}^{b} \mathbb{E}^{p}\left[ \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) \right]~\mathrm{d}t \notag \\
%        &\leq c \max\left\{ \sqrt{\frac{p}{n_{\mtx{\Psi}}}}, \frac{p}{n_{\mtx{\Psi}}}  \right\} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t .
%    \end{align}
%    Consequently, by Markov's inequality,  the inequality 
%    \begin{equation} 
%        \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t \leq c \gamma \max\left\{ \sqrt{\frac{p}{n_{\mtx{\Psi}}}}, \frac{p}{n_{\mtx{\Psi}}}  \right\} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t
%    \end{equation}
%    holds for any $\gamma \geq 1$ with probability at least $1 - \gamma^{-p}$. 
%
%    Setting $\gamma = e$ and choosing $p = \log(\delta^{-1}) \geq 1$, the second part of the theorem follows when taking $n_{\mtx{\Psi}} = \lceil c^2 e^2 \log(\delta^{-1}) \varepsilon^{-2} \rceil$ and restricting $\varepsilon \leq c e$.
%    %where $\lfloor x \rfloor$ is the floor function which returns the largest integer smaller than or equal to %$x$,
%    %we get that for all $\delta \in (0, 1)$
%\end{proof}

\refthm{thm:hutchinson} coincides with the analogous result for the constant case~\cite[Lemma 2.1]{meyer-2021-hutch-optimal}, except for a slightly more restricted choice of $\delta$.

\begin{remark}
    A slightly different version of \refthm{thm:hutchinson} can be derived when $\mtx{B}(t)$ is nonzero and symmetric positive semi-definite for all $t \in [a, b]$: after dividing both sides of \refequ{equ:moment-bound-hutchinson} by $\Trace(\mtx{B}(t))$, one can apply the remaining steps in the proof to deduce with probability at least $1 - \delta$
    \begin{equation}
        \int_{a}^{b} \frac{| \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) |}{\Trace(\mtx{B}(t))} ~\mathrm{d}t < \varepsilon,
    \end{equation}
    if $n_{\mtx{\Psi}} \geq 2 c^2 e^2 \log(\delta^{-1}) \varepsilon^{-2} \rho^2$, where we identify
    \begin{equation}
        \rho = \int_{a}^{b} \frac{\lVert \mtx{B}(t) \rVert _F}{\Trace(\mtx{B}(t))} ~\mathrm{d}t
    \end{equation}
    The quantity $\rho$ is small when on the interval $[a, b]$ the singular values of $\mtx{B}(t)$ decay slowly and large when they decay quickly. This parallels a similar result for constant matrices in~\cite[Remark 2]{cortinovis-2022-randomized-trace}.
\end{remark}

\color{blue}

%The idea behind the proof is to bound  to extend the results to the continuous parameter case. We first only consider the $1$-query estimator and subsequently use a diagonal embedding trick to carry the result over to the general case.
%\begin{proof} 
%\textcolor{red}{??? I feel that the block diagonal embedding trick should come first, like in the proof of Meyer et al. Hopefully, this gets rid of the additional $\log \delta$ factor ???}
%The proof proceeds by bounding moments for fixed values of $t$ and then use Minkowski's integral inequality. We start by considering the residual of the 1-query Girard-Hutchinson estimator for a fixed matrix $\mtx{B}$: 
%    \begin{equation}
%        r(\mtx{B}, \vct{\psi}) := \Trace(\mtx{B}) - \vct{\psi}^{\top} \mtx{B} \vct{\psi}
%    \end{equation}
%%    for a symmetric parameter-dependent matrix $\mtx{B}(t)$ over the real numbers
%for a Gaussian random vector $\vct{\psi}$.
%%
%%    First, we consider $r(\mtx{B}(t), \vct{\psi})$ for a fixed $t \in [a,b]$ and therefore temporarily ignore the parameter-dependence.
%The proof of \cite[Lemma 3]{cortinovis-2022-randomized-trace} shows that $r(\mtx{B}, \vct{\psi})$ is sub-Gamma with parameters $(v, c) = (2 \lVert \mtx{B} \rVert _F^2, 2 \lVert \mtx{B} \rVert _2)$ where $\lVert \cdot \rVert _2$ denotes the spectral norm. By \cite[Theorem 2.3]{boucheron-2013-concentration-inequalities} this implies for every $k \in \mathbb{N}$ that
%    \begin{align*}
%        \mathbb{E}\left[ r(\mtx{B}, \vct{\psi})^{2 k} \right]
%        &\leq k! \left( 16 \lVert \mtx{B} \rVert _F^2 \right)^k + (2 k)! \left( 8 \lVert \mtx{B} \rVert _2 \right)^{2 k}  \\
%        &\le \big( k! 2^{4 k} + (2 k)! 2^{6 k}\big) \lVert \mtx{B} \rVert _F^{2 k} 
%        \le \frac{9}{8} (2 k)! 2^{6 k} \lVert \mtx{B} \rVert _F^{2k}.
%    \end{align*}
%%     Because $\lVert \mtx{B} \rVert _2 \leq \lVert \mtx{B} \rVert _F$ and $k! 2^{4 k} + (2 k)! 2^{6 k} \leq \frac{9}{8}(2 k)! 2^{6 k}$ for any $k \in \mathbb{N}$, we can upper bound 
%%     \begin{equation}
%%         \mathbb{E}\left[ r(\mtx{B}, \vct{\psi})^{2 k} \right] \leq 
%%     \end{equation}
%    Stirling's approximation \cite{robbins-1955-remark-stirling} gives $(2 k)! < 2 \sqrt{\pi k}  e^{\sfrac{1}{24 k}} ( 2 k / e )^{2 k}$. \textcolor{red}{??? this norm should be defined more prominently somewhere else Consequently, the $L^{2k}$ norm $\mathbb{E}^{2k}[\cdot] = \left(\mathbb{E}\left[ | \cdot |^{2k} \right] \right)^{\sfrac{1}{2k}}$ ??? } \textcolor{green}{Moved it to start of section.} Consequently, the high order moments of $r(\mtx{B}, \vct{\psi})$ are bounded by
%    \begin{equation}
%        \mathbb{E}^{2k}\left[ r(\mtx{B}, \vct{\psi}) \right]
%        \leq \left( \frac{9}{4} \sqrt{\pi k} e^{\sfrac{1}{24 k}} \right)^{\sfrac{1}{2k}} \left( \frac{2 k}{e}\right) 2^{3} \lVert \mtx{B} \rVert _F \leq 2^4 k \lVert \mtx{B} \rVert _F,
%        \label{equ:hutchinson-trace-onequery-fixed}
%    \end{equation}
%    using the inequality $(\frac{9}{4} \sqrt{\pi k} e^{\sfrac{1}{24 k}})^{\sfrac{1}{2k}} < e$, which can be verified using the monotonic decrease of the left-hand side with respect to $k \in \mathbb{N}$.
%
%    To address the parameter-dependent case, we first note that the continuity assumption implies that $r(\mtx{B}(t), \vct{\psi})$ is measurable. Therefore, we can apply Minkowski's integral inequality~\cite[Theorem 202]{hardy-1952-inequalities} to conclude from~\refequ{equ:hutchinson-trace-onequery-fixed} that
%    \begin{equation}
%        \mathbb{E}^{2 k}\left[ \int_{a}^{b} |r(\mtx{B}(t), \vct{\psi})|~\mathrm{d}t  \right]
%        \leq \int_{a}^{b} \mathbb{E}^{2 k}\left[ r(\mtx{B}(t), \vct{\psi}) \right]~\mathrm{d}t
%        \leq 2^4 k \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t.
%    \end{equation}
%    Consequently, by Markov's inequality,  the inequality 
%    \begin{equation} 
%        \int_{a}^{b} |r(\mtx{B}(t), \vct{\psi})|~\mathrm{d}t \leq 2^4 k \gamma \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t.
%        \label{equ:hutchinson-trace-onequery-uniform}
%    \end{equation}
%    holds for any $\gamma \geq 1$ with probability at least $1 - \gamma^{-2 k}$. 
%
%    To extend the bound~\refequ{equ:hutchinson-trace-onequery-uniform} from $1$ to $n_{\mtx{\Psi}}$ queries, we use a standard embedding trick (see, e.g.,~\cite[Theorem 1]{cortinovis-2022-randomized-trace}). Letting
%    \begin{equation}
%        \widetilde{\mtx{B}}(t)
%        = \frac{1}{n_{\mtx{\Psi}}} \begin{pmatrix}
%            \mtx{B}(t) & & \\
%            & \ddots & \\
%            & & \mtx{B}(t)
%        \end{pmatrix}
%        \quad \text{and} \quad
%        \widetilde{\vct{\psi}} = \begin{pmatrix}
%            \vct{\psi}_1 \\
%            \vdots \\
%            \vct{\psi}_{n_{\mtx{\Psi}}}
%        \end{pmatrix},
%    \end{equation}
%    we note that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) = r(\widetilde{\mtx{B}}(t), \widetilde{\vct{\psi}})$ and $\lVert \widetilde{\mtx{B}}(t) \rVert _F = \lVert \mtx{B}(t) \rVert _F / \sqrt{n_{\mtx{\Psi}}}$ hold. Thus, the claimed probabilistic bound~\refequ{eq:ghbound} follows from applying~\refequ{equ:hutchinson-trace-onequery-uniform} to $r(\widetilde{\mtx{B}}(t), \widetilde{\vct{\psi}})$.
%%     Then 
%%     where $\vct{\psi}_1, \dots, \vct{\psi}_{n_{\mtx{\Psi}}} \in \mathbb{R}^{n}$ are independent Gaussian random vectors. Then $\lVert \widetilde{\mtx{B}}(t) \rVert _F = \lVert \mtx{B}(t) \rVert _F / \sqrt{n_{\mtx{\Psi}}}$, and
%%     \begin{equation}
%%         r(\widetilde{\mtx{B}}(t), \widetilde{\vct{\psi}}) = \Trace(\mtx{B}(t)) - \frac{1}{n_{\mtx{\Psi}}} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j.
%%     \end{equation}
%%     Hence, by applying \refequ{equ:hutchinson-trace-onequery-uniform} to $r(\widetilde{\mtx{B}}(t), \widetilde{\vct{\psi}})$ we conclude that for any $k \in \mathbb{N}$ and $\gamma \geq 1$ with probability at least $1 - \gamma^{-2 k}$
%%     \begin{equation}
%%         \int_{a}^{b} \left| \Trace(\mtx{B}(t)) - \frac{1}{n_{\mtx{\Psi}}} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j \right| ~ \mathrm{d}t
%%         \leq 2^4 k \gamma \frac{1}{\sqrt{n_{\mtx{\Psi}}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t.
%%     \end{equation}
%
%    %Setting $\delta = \gamma^{-2 k}$ and choosing $k = \lceil \log(\delta^{-1}) %\rceil$, where $\lceil x \rceil$ is the ceiling function which returns the greatest %integer larger than or equal to $x$, we get that for all $\delta \in (0, e^{-\sfrac%{1}{2}})$
%    %\begin{equation}
%    %    k \gamma = \lceil \log(\delta^{-1}) \rceil \delta^{-\sfrac{1}{2\lceil \log%(\delta^{-1}) \rceil}}
%    %    = \lceil \log(\delta^{-1}) \rceil e^{\sfrac{1}{2}\frac{\log(\delta^{-1})}%{\lceil \log(\delta^{-1}) \rceil}}
%    %    \leq 2 \log(\delta^{-1}) e^{\sfrac{1}{2}},
%    %\end{equation}
%    %from which follows the second part of the theorem when $n_{\mtx{\Psi}} = (2^5 e^%{\sfrac{1}{2}} \varepsilon^{-1} \log(\delta^{-1}))^2$.
%
%    Setting $\gamma = e$ and choosing $k = \lfloor \log(\delta^{-1}) / 2 \rfloor$,
%    %where $\lfloor x \rfloor$ is the floor function which returns the largest integer smaller than or equal to %$x$,
%    %we get that for all $\delta \in (0, 1)$
%    the second part of the theorem follows when taking $n_{\mtx{\Psi}} = \lceil (2^3 e \varepsilon^{-1} \log(\delta^{-1}))^2 \rceil$.
%    %where $\lceil x \rceil$ is the ceiling function which returns the smallest integer larger than or equal to $x$.
%    %Consequently, with probability $1 - \delta$ we have
%    %\begin{equation}
%    %    \int_{a}^{b} \left| \frac{1}{n_{\mtx{\Psi}}} \sum_{i=1}^{n_{\mtx{\Psi}}} \vct{\psi}_i^{\top} \mtx{B}(t) \vct{\psi}_i - \Trace(\mtx{B}(t)) \right| ~ \mathrm{d}t \leq 32 e^{\sfrac{1}{2}} \frac{\log(\delta^{-1})}{\sqrt{n_{\mtx{\Psi}}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F~\mathrm{d}t
%    %\end{equation}
%\end{proof}
%
%Compared to the analogous result for the constant case~\cite[Lemma 2.1]{meyer-2021-hutch-optimal}, the number of queries $n_{\mtx{\Psi}}$ needed by~\refthm{thm:hutchinson} is roughly a factor $\log(\delta^{-1})$ larger.
%
\subsubsection{Nyström approximation}
\label{subsec:nystrom}

As a consequence of~\cite{kressner-2024-randomized-lowrank}, the following result confirms the inituition that the Nystr\"om approximation is very well suited in the presence of decaying singular values: If the nuclear norm error to the best rank-$r$ approximation to $B(t)$ is bounded by $\varepsilon$ for every $t$ then the trace of the Nystr\"om approximation (with a bit of oversampling) results, with high probability, in an error that is only modestly larger than $\varepsilon$.

\begin{theorem}{Nyström estimator for parameter-dependent matrices}{nystrom}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be symmetric positive semi-definite and continuous for $t \in [a, b]$. For integers $r \geq 2$ and $n_{\mtx{\Omega}} \geq r + 4$, consider the Nystr\"om approximation $\Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ defined in~\refequ{equ:nystrom-approximation} with an $n\times n_{\mtx{\Omega}}$ Gaussian random matrix $\mtx{\Omega}$. Then for any $\gamma \geq 1$ the bound 
    \[
        \int_{a}^{b} \big| \Trace(\mtx{B}(t)) - \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big| ~\mathrm{d}t
        \leq \gamma^2 (1 + r) \int_{a}^{b} \sum_{i = r+1}^{n} \sigma_i(\mtx{B}(t)) ~\mathrm{d}t.
    \]
    holds with probability at least $1 - \gamma^{-(n_{\mtx{\Omega}} - r)}$.
\end{theorem}
\begin{proof}
Using that $\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ is positive semi-definite (see, e.g.,~\cite[Lemma 2.1]{frangella-2023-randomized-nystrom}) we obtain
    \begin{align*}
        \big| \Trace(\mtx{B}(t)) - \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big|
        & = \big| \Trace(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big|
        = \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _{\ast} \\
        & = \lVert (\mtx{I}_n - \mtx{\Pi}_{\mtx{B}(t)^{\sfrac{1}{2}} \mtx{\Omega}}) \mtx{B}(t)^{\sfrac{1}{2}} \rVert _F^2,
    \end{align*}
    where $\lVert \cdot \rVert _{\ast}$ denotes the nuclear norm and we used \cite[Theorem 1]{gittens-2011-spectral-norm} in the last equality. This allows us to apply \cite[Theorem 5]{kressner-2024-randomized-lowrank}, which immediately implies the statement claimed by the theorem.
\end{proof}

Compared to the constant case~\cite[Theorem 8.1]{tropp-2023-randomized-algorithms}, the bound of \refthm{thm:nystrom} features an additional factor $1+r$ that cannot be compensated with oversampling.

\subsection{Nyström++ estimator for parameter-dependent matrices}
\label{subsec:nystrom-pp}


\refthm{thm:hutchinson} shows that the the Girard-Hutchinson estimator achieves a relative $\varepsilon$-error when using $\mathcal{O}(\varepsilon^{-2})$ queries. The aim of this section is to show that the Nyström++ estimator~\refequ{equ:nystrompp-trace-estimator} improves this to $\mathcal{O}(\varepsilon^{-1})$ queries (that is, matrix-vector products with random vectors), \emph{without} requiring any assumption on the singular values of $\mtx{B}(t)$. This parallels existing results for the constant case for the Hutch++~\cite[Theorem 3.1]{meyer-2021-hutch-optimal} and Nyström++ estimators \cite[Theorem 3.4]{persson-2022-improved-variants}. \textcolor{red}{?? Include specific result ???} \textcolor{green}{Specifically, a bound of the form $|\Trace(\mtx{B}) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B})| \leq \varepsilon \Trace(\mtx{B})$ for any constant symmetric positive semi-definite matrix $\mtx{B}$ is shown to hold with high probability for the Nyström++ estimator $\Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B})$ \refequ{equ:nystrompp-trace-estimator} when $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-1})$ }. Indeed, our proof follows the strategy from~\cite{meyer-2021-hutch-optimal}, by first showing that the parameter-dependent Nyström approximation achieves a relative $\varepsilon$-error when using $\mathcal{O}(\varepsilon^{-2})$ queries.

\color{blue}

\begin{lemma}{Nyström approximation for parameter-dependent matrices}{nystrom}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be symmetric positive semi-definite and continuously depend on $t \in [a, b]$. Consider the Nyström approximation $\Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ with a Gaussian random matrix $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and \emph{even} $n_{\mtx{\Omega}} \geq 4$.
    Then for any $\gamma \geq 1$ the bound
    \begin{equation}
        \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \leq \gamma \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t
        \label{qu:nystrompp-theorem-bound}
    \end{equation}
        holds with probability at least $1 - \gamma^{-\sfrac{n_{\mtx{\Omega}}}{4}}$ for 
        a universal constant $c > 0$. In particular, given $\varepsilon > 0$ and $\delta \in (0, 1)$, the bound $\int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \leq \varepsilon \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t$ holds with probability at least $1-\delta$ if $n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-2} + \log(\delta^{-1}))$.
\end{lemma}

%\todo{Proof idea: structural bound, then higher order moment bound to apply Markov's inequality.}
\begin{proof}
For fixed $t$, consider the spectral decomposition $\mtx{B}(t) = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$ with  
$\mtx{\Lambda} = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$ and $\lambda_1 \ge \dots \ge \lambda_n \ge 0$. Partitioning
    \begin{equation*}
        \rule[\dimexpr-2ex-\ht\strutbox]{0pt}{\dimexpr2ex+4ex+\baselineskip}
        \mtx{U} = \begin{bmatrix}
            \smash{\underbrace{\mtx{U}_1}_{n \times k}} & \smash{\underbrace{\mtx{U}_2}_{n \times (n-k)}}
        \end{bmatrix}
        \quad \text{and} \quad
        \mtx{\Lambda} =
        \begin{bmatrix}
            \smash{\overbrace{\mtx{\Lambda}_1}^{k \times k}} & \\ & \smash{\underbrace{\mtx{\Lambda}_2}_{(n-k) \times (n-k)}}
        \end{bmatrix},
    \end{equation*}
    we set
    $\mtx{\Omega}_1 := \mtx{U}_1^{\top} \mtx{\Omega} \in \mathbb{R}^{k \times n_{\mtx{\Omega}}}$ and $\mtx{\Omega}_2 := \mtx{U}_2^{\top} \mtx{\Omega} \in \mathbb{R}^{(n - k) \times n_{\mtx{\Omega}}}$, which are independent Gaussian random matrices.
Applying Theorem B.1 from~\cite{persson-2023-randomized-lowrank} for $f(x) = x$ yields the bound
    \begin{equation}
        \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}(t)} \rVert _F 
        \leq  \lVert \mtx{\Lambda}_2 \rVert _F + \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2,
        \label{equ:nystrom-proof-persson-bonud}
    \end{equation}
    where $\lVert \cdot \rVert _{(4)}$ denotes the Schatten-4 norm. Standard arguments (see, e.g., the proof of~\cite[Lemma 3]{meyer-2021-hutch-optimal}) provide a simple bound on the first term:
    \begin{equation}
        \lVert \mtx{\Lambda}_2 \rVert _F
        \leq \sqrt{ \lambda_{k+1} (  \lambda_{k+1} + \cdots + \lambda_{n})}
        \leq \sqrt{ \Trace(\mtx{B}) / k \cdot \Trace(\mtx{B})}
        \leq \Trace(\mtx{B}) / \sqrt{k}.
        \label{equ:nystrom-proof-frobenius-trace}
    \end{equation}
    \textcolor{red}{??? STOPPED HERE: Not clear to me why $n_\Omega/4$ should be the optimal exponent for the tail bound ???} \textcolor{green}{I rearranged and reformulated things a bit to hopefully make it clearer:}
    
    \color{black}
    The $q$-th moment of the second term in \refequ{equ:nystrom-proof-persson-bonud} can be processed with standard matrix norm manipulations and the stochastic independence of $\mtx{\Omega}_1$ and $\mtx{\Omega}_2$ to
    \begin{equation}
        \mathbb{E}^{q}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        %&= \mathbb{E}^{p}\left[ \lVert (\mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} )( \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger})^{\top} \rVert _F \right] && \text{($\lVert \mtx{A} \rVert _{(4)}^2 = \lVert \mtx{A} \mtx{A}^{\top} \rVert _{F}$)} \notag \\
        %&\leq \mathbb{E}^{p}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _F \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _2 \right] && \text{($\lVert \mtx{A} \mtx{B} \rVert _F \leq \lVert \mtx{A} \rVert _F \lVert \mtx{B} \rVert _2$)} \notag \\
        \leq \mathbb{E}^{q}\left[ \sqrt{k} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]%  && \text{($\lVert \mtx{A} \rVert _{(4)}^2 \leq \sqrt{k} \lVert \mtx{A} \rVert _2^2$)} \notag \\
        %\leq \mathbb{E}^{p}\left[ \sqrt{k} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2 \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]%  && \text{(submultiplicativity)} \notag \\
        \leq \sqrt{k} \mathbb{E}^{q}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2\right] \mathbb{E}^{q}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2  \right].% && \text{(independence)}
        \label{equ:nystrom-proof-processed-tail}
    \end{equation}
    To match the decay rate of the moments of the first term in \refequ{equ:nystrom-proof-persson-bonud}, we will need to ensure that this term also is of order $\mathcal{O}(\Trace(\mtx{B}) / \sqrt{k})$. We bound the moments of $\lVert \mtx{\Omega}_1^{\dagger} \rVert _2$ similarly to \cite[Lemma B.3]{tropp-2023-randomized-algorithms}, but without restricting $q$ to be smaller than $18$. The explicit integration of \cite[Equation B.7]{tropp-2023-randomized-algorithms} imposes the condition $n_{\mtx{\Omega}} - k - 2q \geq 0$. Both $n_{\mtx{\Omega}}$ and $k$ are integers and must be of the same order to ensure a decay of $\mathcal{O}(\Trace(\mtx{B}) / \sqrt{k})$. To avoid restricting the choice of $n_{\mtx{\Omega}}$ too much, we let it be even and set $k = n_{\mtx{\Omega}}/2$. The moment $q$ should be chosen as large as possible to ensure a fast decay of the failure probability, so we fix it to $q = n_{\mtx{\Omega}}/4$. Therefore, we get
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]
        = \mathbb{E}\left[ \lVert ( \mtx{\Omega}_1 \mtx{\Omega}_1^{\top} )^{-1} \rVert _2^{\sfrac{n_{\mtx{\Omega}}}{4}} \right]^{\sfrac{4}{n_{\mtx{\Omega}}}}%  \notag \\
        \leq \left( 1 + \frac{n_{\mtx{\Omega}}}{2} \right)^{\sfrac{4}{n_{\mtx{\Omega}}}} \left( \frac{1}{(\frac{n_{\mtx{\Omega}}}{2} + 1)!}\right)^{\frac{2}{\frac{n_{\mtx{\Omega}}}{2} + 1}} \left( \frac{3 n_{\mtx{\Omega}}}{4}\right).
    \end{equation}
    From the expansion of the exponential function as a power series $e^n = 1 + n + \dots + n^n/n! + \dots$ for $n \in \mathbb{N}$ we see $e^n \geq 1 + n$ and $e^n \geq n^n / n!$, from which $(1 + n)^{\sfrac{1}{n}} \leq e$ and $\left( 1/n! \right)^{\sfrac{1}{n}} \leq e/n$ follow respectively. Hence,
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]
        \leq e^2 \frac{e^2}{(\frac{n_{\mtx{\Omega}}}{2}+ 1)^2} \left( \frac{3 n_{\mtx{\Omega}}}{4}\right)
        \leq \frac{3 e^4}{n_{\mtx{\Omega}}}.
        \label{equ:pinv-spectral-norm-bound}
    \end{equation}
    As opposed to \cite[Lemma B.3]{tropp-2023-randomized-algorithms}, our bound holds for all even $n_{\mtx{\Omega}} \in \mathbb{N}$, at the cost of a factor of $e^2$.

    The remaining term can dealt with using \reflem{lem:spectral-norm-moment} with $\mtx{A} = \mtx{\Lambda}_2^{\sfrac{1}{2}}$ and $p = n_{\mtx{\Omega}}/2$
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2 \right]
        \leq n_{\mtx{\Omega}} \left( 2 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{2}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \right).
        \label{equ:spectral-norm-bound-applied}
    \end{equation}

    %The $q$-th moment of the second term can be processed with standard matrix norm manipulations and the stochastic independence of $\mtx{\Omega}_1$ and $\mtx{\Omega}_2$ to
    %\begin{equation}
    %    \mathbb{E}^{q}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        %&= \mathbb{E}^{p}\left[ \lVert (\mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} )( \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger})^{\top} \rVert _F \right] && \text{($\lVert \mtx{A} \rVert _{(4)}^2 = \lVert \mtx{A} \mtx{A}^{\top} \rVert _{F}$)} \notag \\
        %&\leq \mathbb{E}^{p}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _F \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _2 \right] && \text{($\lVert \mtx{A} \mtx{B} \rVert _F \leq \lVert \mtx{A} \rVert _F \lVert \mtx{B} \rVert _2$)} \notag \\
    %    \leq \mathbb{E}^{q}\left[ \sqrt{k} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]%  && \text{($\lVert \mtx{A} \rVert _{(4)}^2 \leq \sqrt{k} \lVert \mtx{A} \rVert _2^2$)} \notag \\
        %\leq \mathbb{E}^{p}\left[ \sqrt{k} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2 \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]%  && \text{(submultiplicativity)} \notag \\
    %    \leq \sqrt{k} \mathbb{E}^{q}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2\right] \mathbb{E}^{q}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2  \right].% && \text{(independence)}
    %    \label{equ:nystrom-proof-processed-tail}
    %\end{equation}
    %To match the decay rate of the moments of the first term in \refequ{equ:nystrom-proof-persson-bonud}, we will need to ensure that this term also is of order $\mathcal{O}(\Trace(\mtx{B}) / \sqrt{k})$. To this purpose, and to simplify the notation, we choose $k = n_{\mtx{\Omega}}/2$ and $q = n_{\mtx{\Omega}}/4$ for even $n_{\mtx{\Omega}}$ to ensure $k \in \mathbb{N}$. We can apply \reflem{lem:spectral-norm-moment} with $\mtx{A} = \mtx{\Lambda}_2^{\sfrac{1}{2}}$ and $p = n_{\mtx{\Omega}}/2$ to bound
    %\begin{equation}
    %    \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \rVert _2^2 \right]
    %    \leq n_{\mtx{\Omega}} \left( 2 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{2}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \right).
    %    \label{equ:spectral-norm-bound-applied}
    %\end{equation}
    %The moments of $\lVert \mtx{\Omega}_1^{\dagger} \rVert _2$ can be bound with help of the proof of \cite[Lemma B.3]{tropp-2023-randomized-algorithms}
    %\begin{equation}
    %    \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]
    %    = \mathbb{E}\left[ \lVert ( \mtx{\Omega}_1 \mtx{\Omega}_1^{\top} )^{-1} \rVert _2^{\sfrac{n_{\mtx{\Omega}}}{4}} \right]^{\sfrac{4}{n_{\mtx{\Omega}}}}%  \notag \\
    %    \leq \left( 1 + \frac{n_{\mtx{\Omega}}}{2} \right)^{\sfrac{4}{n_{\mtx{\Omega}}}} \left( \frac{1}{(\frac{n_{\mtx{\Omega}}}{2} + 1)!}\right)^{\frac{2}{\frac{n_{\mtx{\Omega}}}{2} + 1}} \left( \frac{3 n_{\mtx{\Omega}}}{4}\right).
    %\end{equation}
    %From the expansion of the exponential function as a power series $e^n = 1 + n + \dots + n^n/n! + \dots$ for $n \in \mathbb{N}$ we see $e^n \geq 1 + n$ and $e^n \geq n^n / n!$, from which $(1 + n)^{\sfrac{1}{n}} \leq e$ and $\left( 1/n! \right)^{\sfrac{1}{n}} \leq e/n$ follow respectively. Hence,
    %\begin{equation}
    %    \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Omega}_1^{\dagger} \rVert _2^2 \right]
    %    \leq e^2 \frac{e^2}{(\frac{n_{\mtx{\Omega}}}{2}+ 1)^2} \left( \frac{3 n_{\mtx{\Omega}}}{4}\right)
    %    \leq \frac{3 e^4}{n_{\mtx{\Omega}}}.
    %    \label{equ:pinv-spectral-norm-bound}
    %\end{equation}
    Inserting \refequ{equ:spectral-norm-bound-applied} and  \refequ{equ:pinv-spectral-norm-bound} in \refequ{equ:nystrom-proof-processed-tail} with $k = n_{\mtx{\Omega}} / 2$ gives
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{3 e^4}{\sqrt{2}}  \sqrt{n_{\mtx{\Omega}}} \left( 2 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{2}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \right).
        %\leq \sqrt{k} \frac{e^4}{2} \frac{(k + n_{\mtx{\Omega}})(2p + n_{\mtx{\Omega}})}{(n_{\mtx{\Omega}} - k + 1)^2}  \left( 3 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{1}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \right).
    \end{equation}
    We can identify $\lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 = \lambda_{k+1}$ and $\lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 = \Trace(\mtx{\Lambda}_2)$, and just as in the proof of \cite[Lemma 3.1]{meyer-2021-hutch-optimal} we bound $\lambda_{k+1} \leq \Trace(\mtx{B})/k$ and $\Trace(\mtx{\Lambda}_2) \leq \Trace(\mtx{B})$ -- remembering $k = n_{\mtx{\Omega}}/2$ -- to get 
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{3 e^4}{\sqrt{2}}  \sqrt{n_{\mtx{\Omega}}} \left( \frac{4}{n_{\mtx{\Omega}}} \Trace(\mtx{B}) + \frac{2}{n_{\mtx{\Omega}}} \Trace(\mtx{B}) \right)
        \leq 9 \sqrt{2} e^4 \frac{1}{\sqrt{n_{\mtx{\Omega}}}} \Trace(\mtx{B}).
        \label{equ:nystrom-proof-tail-bound}
        %\leq \sqrt{k} \frac{e^4}{2} \frac{(k + n_{\mtx{\Omega}})(2p + n_{\mtx{\Omega}})}{(n_{\mtx{\Omega}} - k + 1)^2}  \left( 3 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{1}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \right).
    \end{equation}
    Inserting \refequ{equ:nystrom-proof-tail-bound} along with \refequ{equ:nystrom-proof-frobenius-trace} in \refequ{equ:nystrom-proof-persson-bonud} and using the triangle inequality for $\mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}[\cdot]$ we obtain
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[\lVert \mtx{B} - \Nystr{\mtx{\Omega}}{\mtx{B}} \rVert _F \right]
        \leq \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[ \lVert \mtx{\Lambda}_2 \rVert _F \right] + \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \Trace(\mtx{B}).
    \end{equation}
    where $c = 1 + 9 \sqrt{2} e^4$.

    As in \cite{kressner-2024-randomized-lowrank}, we can show that the error $\lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F$ is measurable and by the Minkowski's integral inequality \cite[Theorem 202]{hardy-1952-inequalities}, we get for $n_{\mtx{\Omega}} \geq 4$
    \begin{align}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \right] 
        &\leq \int_{a}^{b} \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[\lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F \right] ~\mathrm{d}t \notag \\
        %&\leq \mathbb{E}^{\sfrac{k}{2}} \left[ \frac{1}{\sqrt{k}} \int_{a}^{b} \Trace(\mtx{B}(t)) + \sqrt{k} \lVert \mtx{\Lambda}_2(t)^{\sfrac{1}{2}} \mtx{\Omega}_2(t) \mtx{\Omega}_1(t)^{\dagger} \rVert _2^2~\mathrm{d}t  \right] && \text{(\refequ{xy})} \notag \\
        %&\leq \frac{1}{\sqrt{k}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t + \mathbb{E}^{\sfrac{k}{2}} \left[ \int_{a}^{b}  \sqrt{k} \lVert \mtx{\Lambda}_2(t)^{\sfrac{1}{2}} \mtx{\Omega}_2(t) \mtx{\Omega}_1(t)^{\dagger} \rVert _2^2~\mathrm{d}t  \right] \notag \\
        %&\leq \frac{1}{\sqrt{k}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t + \sqrt{k} \int_{a}^{b} \mathbb{E}^{\sfrac{k}{2}} \left[ \lVert \mtx{\Lambda}_2(t)^{\sfrac{1}{2}} \mtx{\Omega}_2(t) \mtx{\Omega}_1(t)^{\dagger} \rVert _2^2 \right]  ~\mathrm{d}t \notag \\
        &\leq \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t.
    \end{align}
    From Markov's inequality follows with probability at least $1 - \gamma^{-\sfrac{n_{\mtx{\Omega}}}{4}}$
    \begin{equation}
        \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \leq \gamma \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t,
    \end{equation}
    from which follows the first statement.

    Fixing $\gamma = e$ and letting $n_{\mtx{\Omega}} = \lceil(c e \varepsilon)^{-2}  + 4 \log(\delta^{-1}) \rceil$ gives us the second part of the theorem.

    %Setting $\delta = \gamma^{-\sfrac{k}{4}}$ and choosing $k = \lceil \varepsilon^{-2}\log(\delta^{-1}) \rceil$ we get
    %\begin{align}
    %    \gamma \frac{1}{\sqrt{k}}
    %    &= \delta^{-\sfrac{2}{\lceil \varepsilon^{-2} \log(\delta^{-1}) \rceil}} \frac{1}{\sqrt{\lceil \varepsilon^{-2} \log(\delta^{-1}) \rceil}} && \text{(definition of $\delta$ and choice of $k$)} \notag \\
    %    &= e^{2\varepsilon^{2} \frac{\varepsilon^{-2}\log(\delta^{-1})}{\lceil \varepsilon^{-2} \log(\delta^{-1}) \rceil}} \frac{1}{\sqrt{\lceil \varepsilon^{-2} \log(\delta^{-1}) \rceil}} && \text{($\delta = e^{-\log(\delta^{-1})}$)} \notag \\ 
    %    &\leq e^{2\varepsilon^{2}} \frac{\varepsilon}{\sqrt{ \log(\delta^{-1})}} && \text{($\lceil x \rceil \geq x$ if $x \geq 0$)}
    %\end{align}
    %which is smaller than $\varepsilon$ if $\delta \leq e^{-e^{4 \varepsilon^2}}$from which follows the second part of the theorem when using $n_{\mtx{\Omega}} = 2k$.
\end{proof}

\color{black}


When compared to the equivalent result for constant matrices \cite[Lemma 3.2]{persson-2022-improved-variants}, our constant factor up front is a bit more than two times larger and our failure probability decays half a power slower.
 
Finally, we can now combine the bound on the Girard-Hutchinson estimator (\refthm{thm:hutchinson}) and the one on the Nyström approximation (\reflem{lem:nystrom}) to obtain our main result.

\begin{theorem}{Nyström++ trace estimator for parameter-dependent matrices}{nystrom-pp}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be symmetric positive semi-definite and continuously depend on $t \in [a, b]$. If $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-1} \log(\delta^{-1}))$ with \emph{even} $n_{\mtx{\Omega}} \geq 4$, then for $\delta \in (0, 2e^{-1})$ and $\varepsilon \in (0, 1)$ with probability at least $1 - \delta$ 
    \begin{equation}
        \int_{a}^{b} | \Trace(\mtx{B}(t)) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) |~\mathrm{d}t
        \leq \varepsilon \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t.
    \end{equation}
\end{theorem}

\begin{proof}
    By choosing $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\tilde{\varepsilon}^{-2} \log(\tilde{\delta}^{-1}))$ we get
    \begin{align}
        &\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) |~\mathrm{d}t \notag \\
        &= \int_{a}^{b} | \Trace(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) |~\mathrm{d}t && \text{(by definition of estimators)} \notag \\
        &\leq \tilde{\varepsilon} \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F ~\mathrm{d}t && \text{(\refthm{thm:hutchinson} w.p. $\geq 1 - \tilde{\delta}$)} \notag \\
        &\leq \tilde{\varepsilon}^2 \int_{a}^{b} \Trace(\mtx{B}(t)) ~\mathrm{d}t && \text{(\reflem{lem:nystrom} w.p. $\geq 1 - \tilde{\delta}$)} 
    \end{align}
    with probability at least $1 - 2\tilde{\delta}$ by a union bound. Taking $\varepsilon = \tilde{\varepsilon}^2$ and $\delta = 2 \tilde{\delta}$ we conclude the result.

\end{proof}

Comparing \refthm{thm:nystrom-pp} with its equivalent for constant matrices \cite[Theorem 3.4]{persson-2022-improved-variants}, we notice an additional factor of $\sqrt{\log(\delta^{-1})}$ in the required number of random vectors $n_{\mtx{\Omega}}$ and $n_{\mtx{\Psi}}$.

% \begin{proof}
%     Combining with union bound.
%     \begin{align}
%         &\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) |~\mathrm{d}t \notag \\
%         &= \int_{a}^{b} | \Hutch{\mtx{\Psi}}(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{A}}(t)) - \Trace(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{A}}(t)) |~\mathrm{d}t && \text{(definition of estimators)} \notag \\
%         &= \todo{c} \frac{1}{\sqrt{n_{\mtx{\Psi}}}} \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{A}}(t) \rVert _F ~\mathrm{d}t && \text{(with probability $1 - \delta$ if $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-1})$)} \notag \\
%         &= \todo{c} \frac{1}{\sqrt{n_{\mtx{\Psi}} n_{\mtx{\Omega}}}} \int_{a}^{b} \Trace(\mtx{B}(t)) ~\mathrm{d}t
%     \end{align}
% \end{proof}
