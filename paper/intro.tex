\section{Introduction}
\label{sec:introduction}

Spectral distributions can reveal important properties in numerous applications across physics, chemistry, engineering, and data science: For example, in electronic structure calculations eigenvalues represent the energy levels which electrons occupy\cite{drabold-1993-maximum-entropy, ducastelle-1970-moments-developments, haydock-1972-electronic-structure, lin-2017-randomized-estimation}, in machine learning they are indicative of the topology of the loss landscape \cite{ghorbani-2019-investigation-neural, yao-2020-pyhessian-neural}, and in graph processing they can uncover hidden graph motifs~\cite{huang-2021-density-states}. Given a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$, the distribution of its eigenvalues $\lambda_1, \dots, \lambda_n \in \mathbb{R}$ can be represented by the spectral density $\phi(t) = n^{-1} \sum_{i=1}^{n} \delta(t - \lambda_i)$, which places a rescaled Dirac delta distribution $\delta$ at each eigenvalue. Clearly, assembling this expression amounts to computing all eigenvalues of the matrix; an operation that is often prohibitively expensive. Several techniques have been developed for approximating $\phi$, including moment matching~\cite{cohen-steiner-2018-approximating-spectrum, braverman-2022-sublinear-time} and polynomial approximation, either implicitly by a Lanczos procedure \cite{lin-2016-approximating-spectral, chen-2021-analysis-stochastic}, or explicitly by expansion \cite{weisse-2006-kernel-polynomial, lin-2016-approximating-spectral}.

Being composed of distributions, the function $\phi$ itself is hard to approximate. However, as many application only require a rough estimate of $\phi$, it usually suffices to instead approximate a smoothed spectral density of the form
\begin{equation}
    \phi_{\sigma}(t) = \frac{1}{n} \sum_{i=1}^{n} g_{\sigma}(t - \lambda_i) = \frac{1}{n} \Trace(g_{\sigma}(t \mtx{I}_n - \mtx{A})),
    \label{equ:smoothed-spectral-density}
\end{equation}
for some fixed smoothing kernel $g_{\sigma}$, typically a Gaussian~\cite{lin-2016-approximating-spectral, lin-2017-randomized-estimation} or a Lorentzian~\cite{haydock-1972-electronic-structure, lin-2016-approximating-spectral}. We thus arrive at the problem of computing the trace of the parameter-dependent matrix function $\mtx{B}(t) \equiv g_{\sigma}(t \mtx{I}_n - \mtx{A}) \in \mathbb{R}^{n \times n}$. Evaluating this matrix function \emph{exactly} requires the diagonalization of $\mtx{A}$, which does not seem to yield any gains compared to the original problem of computing its spectral distribution. The crucial point is that we can now \emph{estimate} the trace by matrix-vector products with $\mtx{B}(t)$, which -- thanks to the smoothness of $g_\sigma$ -- can be well approximated by a few matrix-vector products with $\mtx{A}$.

For a \emph{constant} matrix $\mtx{B}$, one of the most popular trace estimators is the Girard-Hutchinson estimator \cite{girard-1989-fast-montecarlo, hutchinson-1990-stochastic-estimator} along with variance reduction techniques~\cite{gambhir-2017-deflation-method, saibaba-2017-randomized-matrixfree, lin-2017-randomized-estimation, meyer-2021-hutch-optimal, persson-2022-improved-variants, chen-2023-krylovaware-stochastic, epperly-2024-xtrace-making}. Suitable extensions to parameter-dependent matrices have been considered, e.g., in~\cite{lin-2017-randomized-estimation,chen-2023-krylovaware-stochastic}, but we are not aware of an analysis providing insight and justification of these extensions. In passing, we note that dynamic trace estimation~\cite{dharangutte-2024-dynamic-trace,woodruff-2024-optimal-query} is an efficient technique for subsequently estimating the traces of matrices $\mtx{B}(t_1), \dots, \mtx{B}(t_m)$ for wich the increments $\mtx{B}(t_{i+1}) - \mtx{B}(t_i)$ are relatively small in norm. The potential of dynamic trace estimation appears to be limited in our setting because $\mtx{B}(t)$ may change rapidly close to eigenvalues, with $g_{\sigma}$ approximating a Dirac delta function.

All methods considered in this work are based on the following simple idea: Apply an existing randomized trace estimator to $\Trace(\mtx{B}(t))$ with \emph{constant} random vectors, that is, the same randomization is used for each value of the parameter $t$. \textcolor{green}{For example, the Girard-Hutchinson estimator becomes $n_{\mtx{\Psi}}^{-1} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j$ for $n_{\mtx{\Psi}}$ constant standard Gaussian random vectors $\vct{\psi}_1, \dots, \vct{\psi}_{n_{\mtx{\Psi}}}$.}
??? STOP HERE ???


\textcolor{red}{??? The following two paragraphs need to rewritten a bit at a later stage. It needs to be clearer what the approaches proposed in our paper are really about and the novelty wrt existing work has to come out more clearly and concretely. ??? Mention \cite{park-2024-lowrank-approximation} ?.}
\color{black}
Instead of discretizing $[a, b]$ into $t_1, \ldots, t_m$, approximating $\Trace(\mtx{B}(t_i))$ with $b(t_i)$, and analyzing the error $|\Trace(\mtx{B}(t_i)) - b(t_i)|$ for each $i$, a more natural way is to quantify the error with $\int_{a}^{b} | \Trace(\mtx{B}(t)) - b(t) |~\mathrm{d}t$, where $b(t)$ is the trace estimate in each $t \in [a, b]$. This approach has already been used in the analysis of low-rank approximations to parameter-dependent matrices \cite{kressner-2024-randomized-lowrank}.

\paragraph{New contributions.} We analyze three well-established randomized trace estimators when they are applied to parameter-dependent matrices: The Girard-Hutchinson estimator \cite{girard-1989-fast-montecarlo, hutchinson-1990-stochastic-estimator}, the trace of the Nyström low-rank approximation \cite{gittens-2013-revisiting-nystrom}, and the Nyström++ estimator \cite{persson-2022-improved-variants}. Interestingly, we can reuse the same randomization for computing the estimate at each value of the parameter $t$, which makes the estimators scale favorably with the number of parameter evaluations. We propose certain modifications to the methods from \cite{lin-2017-randomized-estimation} for approximating the smoothed spectral density $\phi_{\sigma}$ \refequ{equ:smoothed-spectral-density}. In particular, we use a more rigorous approach for expanding matrix functions in terms of Chebyshev polynomials and unify all of their methods in a single algorithm, with the desirable side effect of speeding up their \enquote{spectrum sweeping} method by orders of magnitude. Based on the analysis of the randomized trace estimators for parameter-dependent matrices, we provide bounds on the approximation error for our method.

\paragraph{Structure.} In \refsec{sec:analysis} we will analyze three standard trace estimators -- the Girard-Hutchinson, Nyström, and Nyström++ estimators -- when they are applied to parameter-dependent matrices. We propose and analyze an algorithm for using the aforementioned estimators to approximate the spectral density of real symmetric matrices in \refsec{sec:application}. Finally, we will test our developments on an example from electronic structure calculation in \refsec{sec:results}.

\paragraph{Reproducibility.} \input{re-pro-badge.tex}
