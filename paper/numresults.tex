
\section{Numerical experiments}
\label{sec:results}

In this last section, we test our algorithm in multiple scenarios. First, we numerically confirm our theoretical convergence bounds by approximating the spectral density of a Hamiltonian arising from electronic structure interaction. Subsequently, we compare our method to the Krylov-aware stochastic trace estimator \cite{chen-2023-krylovaware-stochastic} once on the electronic structure example and once for estimating the trace of a more general parameter-dependent matrix appearing in statistical thermodynamics. In the end, we demonstrate that our algorithm can also be used to monitor the optimization process of a neural network.

\subsection{Spectral density for Hamiltonian of electronic structure}
\label{subsec:hamiltonian}

Our first numerical example comes from electronic structure interaction \cite{lin-2017-randomized-estimation}, more precisely from the second order finite difference discretization of the Hamiltonian
\begin{equation}
    \mathcal{H} = - \Delta + V
    \label{equ:5-experiments-electronic-hamiltonian}
\end{equation}
in three dimensions. The potential $V$ interacting with the electrons is generated by Gaussian wells
\begin{equation}
    v(r) = v_0 e^{-\lambda r^2}
    \label{equ:5-experiments-gaussian-cell}
\end{equation}
with $v_0 = -4$ and $\lambda = 8$, centered in cells of side length $L=6$ which are stacked $n_c \in \mathbb{N}$ times in each spatial dimension (cf. \reffig{fig:gaussian-well}). The discretization step is fixed to be $h=0.6$, such that the size of the matrix grows cubically with $n_c$. This is an idealized model for the interaction of nuclei on a regular grid with electrons for a $k$-vector in the center of the first Brillouin zone. The distribution of the eigenvalues of the Hamiltonian -- its spectral density -- allows us to interpret the system's energy levels.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-1.pgf}
        \caption{$n_c=1$}
        \label{fig:gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-2.pgf}
        \caption{$n_c=2$}
        \label{fig:gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-5.pgf}
        \caption{$n_c=5$}
        \label{fig:gaussian-well-5}
    \end{subfigure}
    \caption{Cross-sections of the periodic Gaussian well potential $V$ for different sizes $n_c$ of the supercell.}
    \label{fig:gaussian-well}
\end{figure}

While theoretically an important tool, we waive using the non-negative Chebyshev expansion, since the slight indefiniteness of $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ \refequ{equ:matrix-expansion} does not make a difference in finite precision arithmetic. We approximate the integrals for computing the $L^1$-errors in \refsec{sec:analysis} using a midpoint quadrature with $n_t \in \mathbb{N}$ nodes. For a fixed total number of random vectors $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}$, we analyze the convergence behavior of the Chebyshev-Nyström++ method when run on the $1000 \times 1000$ matrix resulting from the finite difference discretization of the Hamiltonian for $n_c = 1$ described above (see \reffig{fig:convergence}). 

\begin{figure}[ht]
    \centering
    \input{plots/convergence.pgf}
    \caption{For increasing values of $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}$ but fixed $m$ we plot the $L^1$-approximation error for $\sigma=0.005$ on the model problem with $n_c = 1$. The $L^1$-error is approximated by a quadrature with $n_t = 100$ evenly spaced nodes.}
    \label{fig:convergence}
\end{figure}

Indeed, we observe that for $n_{\mtx{\Omega}} = 0$, we require $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2})$ to achieve an error of order $\varepsilon$, as shown in \refthm{thm:hutchinson}. Because the eigenvalues of the matrix are quite uniformly distributed, we observe the stronger convergence of the Nyström approximation discussed in the end of \refsec{sec:application} once $n_{\mtx{\Omega}}$ exceeds the numerical rank \refequ{equ:gaussian-kernel-numerical-rank-uniform}, which in this case would be somewhat larger than $30$.

For a fixed budget of random vectors $n_{\mtx{\Omega}} + n_{\mtx{\Psi}}$, \reffig{fig:distribution} visualizes how for different values of the smoothing parameter $\sigma$ the random vectors should either be invested into the Nyström approximation $n_{\mtx{\Omega}}$ or the estimation of the residual trace $n_{\mtx{\Psi}}$. Again, since for small $\sigma$ the matrix function $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ has a small numerical rank, a Nyström approximation alone is already enough to achieve a good approximation. On the other hand, when $\sigma$ is large, the Nyström approximation by itself is not effective, and the correction with the residual trace becomes indispensable.

\begin{figure}[ht]
    \centering
    \input{plots/distribution.pgf}
    \caption{The Chebyshev-Nyström++ method for different ways of allocations a total of $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}=80$ random vectors to either the Nystr\"om low-rank approximation or the Girard-Hutchinson trace estimation for the Gaussian smoothing kernel with multiple different values of the smoothing parameter. We make the approximation error made in the Chebyshev expansion negligible by rescaling $m=16 / \sigma$ (based on \refthm{thm:chebyshev-error}).}
    \label{fig:distribution}
\end{figure}

\subsection{Comparison to Krylov-aware stochastic trace estimation}
\label{subsec:krylov-aware}

The Krylov-aware stochastic trace estimation \cite{chen-2023-krylovaware-stochastic} is also well suited for approximating the trace of a matrix function $f(\mtx{A}, t)$ which separately depends on a parameter $t$. It also samples two standard Gaussian random matrices $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$. $\mtx{\Omega}$ is used to extract a low-rank approximation to $f(\mtx{A}, t)$ from a block-Krylov subspace of $\mtx{A}$ generated from $r$ iterations of the block-Lanczos method with reorthogonalization and subsequently $q$ without. $\mtx{\Psi}$ is then used to estimate the trace of the approximation residual with $r$ iterations of the Lanczos method.

We run an optimized implementation of the Krylov-aware stochastic trace estimator \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} in multiple configurations on the example from \refsec{subsec:hamiltonian} and plot the approximation errors for logarithmically spaced values of the smoothing parameter $\sigma$ in \reffig{fig:krylov-aware-density}. For reference, we add the error of the Chebyshev-Nyström++ method on parameters which lead to a comparable run-time.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_density_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_density_CN.tex}
    \end{minipage}
    \caption{For the example from \refsec{subsec:hamiltonian} we compare the Chebyshev-Nyström++ (CN++) method with the Krylov-Aware (KA) stochastic trace estimator in multiple configurations. To this extent, we compute the $L^1$-error of the methods for various levels of smoothing $\sigma$ (left). The parameters of the algorithms and run-time averaged over all values of $\sigma$ can be found in the tables (right).}
    \label{fig:krylov-aware-density}
\end{figure}

To demonstrate that the Chebyshev-Nyström++ estimator is also effective on problems unrelated to spectral density estimation, we consider the problem of approximating $\Trace(\exp(-\beta \mtx{A}))$ for the Hamiltonian $\mtx{A}$ of a planar Heisenberg spin chain \cite{chen-2023-krylovaware-stochastic}. We compute the pointwise error from the analytic solution as a function of the temperature $\beta^{-1}$ and compare it to the error achieved by the Krylov-aware trace estimator in \reffig{fig:krylov-aware-spin}.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_spin.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_spin_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_spin_CN.tex}
    \end{minipage}
    \caption{For the Heisenberg spin chain example, we compare the Chebyshev-Nyström++ (CN++) method with the Krylov-Aware stochastic trace estimator in the same configurations as in \cite[Table 5.1]{chen-2023-krylovaware-stochastic} (right). To this extent, we compute the $L^1$-error of the methods for various choices of the temperature parameter $\beta^{-1}$ (left).}
    \label{fig:krylov-aware-spin}
\end{figure}

Unlike in the spectral density example from \refsec{subsec:hamiltonian}, the numerical rank of the matrix we approximate wildly changes with the parameter $\beta$. Since the Chebyshev-Nyström++ method requires a fixed allocation of random vectors to the low-rank approximation for all values of the parameter $\beta$. Hence, both for large $\beta$, where the matrix $\exp(-\beta \mtx{A})$ is low-rank, and for small $\beta$, where the singular values only decay slowly, we cannot expect a good approximation.

\subsection{Spectral density of Hessian matrix of neural network}
\label{subsec:hessian}

When optimizing a neural network over some data, it is of great interest to determine whether a minimum has been found and if this minimum is robust, i.e. a small perturbation of the parameters of the neural network will not lead to a significant decrease in the fit of the neural network. Both properties are reflected in the eigenvalues of the Hessian matrix $\mtx{A}$ with respect to some loss function: if all eigenvalues are non-negative, then the loss attains a local minimum, and if additionally all of them are small, the minimum is robust.

Since neural networks are usually parametrized by thousands of parameters, assembling the Hessian matrix explicitly and then computing its eigenvalues is out of question. Luckily, there exists a method which can exactly compute matrix-vector products $\mtx{A} \vct{x}$ for any vector $\vct{x}$ which scales proportionally with the number of parameters \cite{pearlmutter-1994-fast-exact}.

To demonstrate that our algorithm can effectively be applied in the setting of neural network optimization, we approximate the spectral density of a small fully connected convolutional neural network with 6782 parameters, which we train on the handwritten digit classification task given by the MNIST dataset\footnote{Handwritten digit classification; taken from \url{http://yann.lecun.com/exdb/mnist}.}. We plot the approximation of the spectral density of the Hessian matrix of the untrained neural network, and at different stages of training in \reffig{fig:hessian-density}, as well as the corresponding mean squared error loss. It can well be observed that the eigenvalues creep towards zero as the training proceeds. Furthermore, by the absence of large eigenvalues in the spectrum, we can tell that in the final epoch, the neural network attains a more robust state than in previous epochs.

\begin{figure}
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density_loss.pgf}
    \end{minipage}
    \caption{The mean squared error training loss (right) and the corresponding approximate spectral density of the Hessian matrix of a fully connected convolutional neural network in different epochs of training on the MNIST dataset (left). Computed with the Chebyshev-Nyström++ method (\refalg{alg:nystrom-chebyshev-pp}) on $n_{\mtx{\Omega}} = 10$, $n_{\mtx{\Psi}} = 10$, $m = 1000$, and $\sigma = 0.005$.}
    \label{fig:hessian-density}
\end{figure}
